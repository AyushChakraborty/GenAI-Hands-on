{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2fe75baa",
      "metadata": {
        "id": "2fe75baa"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e80a8ddf",
      "metadata": {
        "id": "e80a8ddf"
      },
      "source": [
        "# Unit 2 - Part 3a: Chain of Thought (CoT)\n",
        "\n",
        "## 1. Introduction: The Inner Monologue\n",
        "\n",
        "Standard LLMs try to jump straight to the answer. For complex problems (math, logic), this often fails.\n",
        "\n",
        "**Chain of Thought (CoT)** forces the model to \"think out loud\" before answering.\n",
        "\n",
        "### Why use a \"Dumb\" Model?\n",
        "For this unit, we will use **Llama3.1-8b** (via Groq). It is a smaller, faster model.\n",
        "Why? Because huge models (like Gemini Pro or GPT-4) are often *too smart*—they solve logic riddles instantly without thinking.\n",
        "\n",
        "To really see the power of Prompt Engineering, we need a model that **needs help**.\n",
        "\n",
        "### Visualizing the Process (Flowchart)\n",
        "```mermaid\n",
        "graph TD\n",
        "    Input[Question: 5+5*2?]\n",
        "    Input -->|Standard| Wrong[Answer: 20 (Wrong)]\n",
        "    Input -->|CoT| Step1[Step 1: 5*2=10]\n",
        "    Step1 --> Step2[Step 2: 5+10=15]\n",
        "    Step2 --> Correct[Answer: 15 (Correct)]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a41e5ba",
      "metadata": {
        "id": "0a41e5ba"
      },
      "source": [
        "## 2. Concept: Latent Reasoning\n",
        "\n",
        "Why does this work?\n",
        "Because LLMs are \"Next Token Predictors\".\n",
        "- If you force it to answer immediately, it must predict the digits `1` and `5` immediately.\n",
        "- If you let it \"think\", it generates intermediate tokens (`5`, `*`, `2`, `=`, `1`, `0`).\n",
        "- The model then **ATTENDS** to these new tokens to compute the final answer.\n",
        "\n",
        "**Writing is Thinking.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ba92b198",
      "metadata": {
        "id": "ba92b198",
        "outputId": "5d1f15de-8bd2-468c-c0ed-5e228d3c49e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnter your Groq API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b (Small/Fast) to demonstrate logic failures\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3088780",
      "metadata": {
        "id": "e3088780"
      },
      "source": [
        "## 3. The Experiment: A Tricky Math Problem\n",
        "\n",
        "Let's try a problem that requires multi-step logic.\n",
        "\n",
        "**Problem:**\n",
        "\"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a70d3b7",
      "metadata": {
        "id": "4a70d3b7",
        "outputId": "d38f7e25-14ee-4f3f-a888-a6a0c45fcc71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- STANDARD (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to add the initial number of tennis balls he had (5) to the number of tennis balls he bought (2 cans * 3 tennis balls per can).\n",
            "\n",
            "2 cans * 3 tennis balls per can = 6 tennis balls\n",
            "\n",
            "Now, let's add the initial number of tennis balls (5) to the number of tennis balls he bought (6):\n",
            "\n",
            "5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "question = \"Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many does he have now?\"\n",
        "\n",
        "# 1. Standard Prompt (Direct Answer)\n",
        "prompt_standard = f\"Answer this question: {question}\"\n",
        "print(\"--- STANDARD (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_standard).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b696ba6",
      "metadata": {
        "id": "5b696ba6"
      },
      "source": [
        "### Critique\n",
        "Smaller models often latch onto the visible numbers (5 and 2) and simply add them (7), ignoring the multiplication step implied by \"cans\".\n",
        "\n",
        "Let's force it to think."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3dd65b0a",
      "metadata": {
        "id": "3dd65b0a",
        "outputId": "9476a9a9-246a-42b9-d436-bf04a13b7c76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Chain of Thought (Llama3.1-8b) ---\n",
            "To find out how many tennis balls Roger has now, we need to follow these steps:\n",
            "\n",
            "1. Roger already has 5 tennis balls.\n",
            "2. He buys 2 more cans of tennis balls. Each can has 3 tennis balls, so he buys 2 x 3 = 6 more tennis balls.\n",
            "3. Now, we add the tennis balls he already had (5) to the tennis balls he bought (6). 5 + 6 = 11\n",
            "\n",
            "So, Roger now has 11 tennis balls.\n"
          ]
        }
      ],
      "source": [
        "# 2. CoT Prompt (Magic Phrase)\n",
        "prompt_cot = f\"Answer this question. Let's think step by step. {question}\"\n",
        "\n",
        "print(\"--- Chain of Thought (Llama3.1-8b) ---\")\n",
        "print(llm.invoke(prompt_cot).content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd205672",
      "metadata": {
        "id": "bd205672"
      },
      "source": [
        "## 4. Analysis\n",
        "\n",
        "Look at the output. By explicitly breaking it down:\n",
        "1.  \"Roger starts with 5.\"\n",
        "2.  \"2 cans * 3 balls = 6 balls.\"\n",
        "3.  \"5 + 6 = 11.\"\n",
        "\n",
        "The model effectively \"debugs\" its own logic by generating the intermediate steps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ee779f",
      "metadata": {
        "id": "22ee779f"
      },
      "source": [
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d1fa7c",
      "metadata": {
        "id": "11d1fa7c"
      },
      "source": [
        "# Unit 2 - Part 3b: Tree of Thoughts (ToT) & Graph of Thoughts (GoT)\n",
        "\n",
        "## 1. Introduction: Beyond A -> B\n",
        "\n",
        "CoT is linear. But complex reasoning is often nonlinear. We need to explore branches (ToT) or even combine ideas (GoT).\n",
        "\n",
        "We continue using **Llama3.1-8b via Groq** to show how structure improves performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4371aa3d",
      "metadata": {
        "id": "4371aa3d"
      },
      "outputs": [],
      "source": [
        "# Setup\n",
        "%pip install python-dotenv --upgrade --quiet langchain langchain-groq\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "import getpass\n",
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "if \"GROQ_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API Key: \")\n",
        "\n",
        "# Using Llama3.1-8b\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0.7) # Creativity needed"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03a348d6",
      "metadata": {
        "id": "03a348d6"
      },
      "source": [
        "## 2. Tree of Thoughts (ToT)\n",
        "\n",
        "ToT explores multiple branches before making a decision.\n",
        "**Analogy:** A chess player considering 3 possible moves before playing one.\n",
        "\n",
        "### Implementation\n",
        "We will generate 3 distinct solutions for a problem and then use a \"Judge\" to pick the best one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1ea2d4c7",
      "metadata": {
        "id": "1ea2d4c7",
        "outputId": "387dd025-b75b-47c9-d7f3-84acd3dd2170",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Tree of Thoughts (ToT) Result ---\n",
            "As a child psychologist, I would recommend **Solution 2: The Storybook Characters** as the most sustainable approach to encouraging a 5-year-old to eat vegetables. Here's why:\n",
            "\n",
            "1. **Long-term engagement**: Storytelling is a powerful tool that can engage children in the long term. By creating a narrative around the vegetables, you can make mealtime a fun and exciting experience that will continue even after the initial novelty wears off.\n",
            "2. **Emotional connection**: Storytelling allows children to form emotional connections with the vegetables, which can increase their willingness to try them. By associating the vegetables with positive characters and stories, you can create a positive emotional connection that will make mealtime more enjoyable.\n",
            "3. **Developmental benefits**: Storytelling is a vital aspect of children's cognitive and language development. By engaging your child in storytelling, you can promote their language skills, imagination, and creativity.\n",
            "4. **Flexibility**: This approach can be adapted to suit your child's interests and preferences. You can create new stories and characters as your child grows and changes, making it a sustainable approach that can evolve with their needs.\n",
            "5. **No reliance on external rewards**: Unlike Solution 3, which might rely on external rewards like stickers or toys, Solution 2 encourages children to eat vegetables because they are invested in the story and the characters. This approach focuses on intrinsic motivation, which is more sustainable and effective in the long term.\n",
            "6. **Promotes self-efficacy**: By involving your child in the process of creating the story and choosing the characters, you can promote their sense of self-efficacy and agency. This can help them feel more confident and capable of trying new foods.\n",
            "\n",
            "In contrast, while Solution 1 (The Rainbow Plate) and Solution 3 (The Vegetable Garden Adventure) can be engaging and fun, they might rely on external rewards or novelty, which can wear off over time. Solution 1, in particular, might become less engaging as children become familiar with the rainbow plate, while Solution 3 might require more maintenance and effort to keep the garden and scavenger hunt interesting.\n",
            "\n",
            "Overall, Solution 2: The Storybook Characters offers a sustainable and effective approach to encouraging children to eat vegetables, as it promotes emotional connection, language development, and self-efficacy while minimizing reliance on external rewards.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "problem = \"How can I get my 5-year-old to eat vegetables?\"\n",
        "\n",
        "# Step 1: The Branch Generator\n",
        "prompt_branch = ChatPromptTemplate.from_template(\n",
        "    \"Problem: {problem}. Give me one unique, creative solution. Solution {id}:\"\n",
        ")\n",
        "\n",
        "branches = RunnableParallel(\n",
        "    sol1=prompt_branch.partial(id=\"1\") | llm | StrOutputParser(),\n",
        "    sol2=prompt_branch.partial(id=\"2\") | llm | StrOutputParser(),\n",
        "    sol3=prompt_branch.partial(id=\"3\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# Step 2: The Judge\n",
        "prompt_judge = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three proposed solutions for: '{problem}'\n",
        "\n",
        "    1: {sol1}\n",
        "    2: {sol2}\n",
        "    3: {sol3}\n",
        "\n",
        "    Act as a Child Psychologist. Pick the most sustainable one (not bribery) and explain why.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# Chain: Input -> Branches -> Judge -> Output\n",
        "tot_chain = (\n",
        "    RunnableParallel(problem=RunnableLambda(lambda x: x), branches=branches)\n",
        "    | (lambda x: {**x[\"branches\"], \"problem\": x[\"problem\"]})\n",
        "    | prompt_judge\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Tree of Thoughts (ToT) Result ---\")\n",
        "print(tot_chain.invoke(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38579cab",
      "metadata": {
        "id": "38579cab"
      },
      "source": [
        "## 3. Graph of Thoughts (GoT)\n",
        "\n",
        "You asked: **\"Where is Graph of Thoughts?\"**\n",
        "\n",
        "GoT is more complex. It's a network. Information can split, process specific parts, and then **AGGREGATE** back together.\n",
        "\n",
        "### The Workflow (Writer's Room)\n",
        "1.  **Split:** Generate 3 independent story plots (Sci-Fi, Fantasy, Mystery).\n",
        "2.  **Aggregate:** The model reads all 3 and creates a \"Master Plot\" that combines the best elements of each.\n",
        "3.  **Refine:** Polish the Master Plot.\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "   Start(Concept) --> A[Draft 1]\n",
        "   Start --> B[Draft 2]\n",
        "   Start --> C[Draft 3]\n",
        "   A & B & C --> Mixer[Aggregator]\n",
        "   Mixer --> Final[Final Story]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "894940b5",
      "metadata": {
        "id": "894940b5",
        "outputId": "a68700dc-02cf-4306-e9b4-57fbdc4237ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Graph of Thoughts (GoT) Result ---\n",
            "Here's a paragraph that combines the technology of sci-fi, the passion of romance, and the fear of horror:\n",
            "\n",
            "\"In 'Timeless Obsession', a brilliant and reclusive physicist, Emma, discovers a way to communicate with her past self using a revolutionary time-traveling device, but her attempts to alter the course of history to be with her lost love, Jack, inadvertently unleash a malevolent entity from the shadows of time. As Emma navigates the blurred lines between past, present, and future, she must confront the dark consequences of her actions, including eerie apparitions, twisted timelines, and the terrifying realization that her obsessive desire to be with Jack may ultimately condemn her to a fate worse than losing him: existing in a reality where he never existed. With each attempt to rewrite history, Emma's reality becomes increasingly distorted, forcing her to make a heart-wrenching choice between her all-consuming passion for Jack and the very fabric of her existence.\"\n"
          ]
        }
      ],
      "source": [
        "# 1. The Generator (Divergence)\n",
        "prompt_draft = ChatPromptTemplate.from_template(\n",
        "    \"Write a 1-sentence movie plot about: {topic}. Genre: {genre}.\"\n",
        ")\n",
        "\n",
        "drafts = RunnableParallel(\n",
        "    draft_scifi=prompt_draft.partial(genre=\"Sci-Fi\") | llm | StrOutputParser(),\n",
        "    draft_romance=prompt_draft.partial(genre=\"Romance\") | llm | StrOutputParser(),\n",
        "    draft_horror=prompt_draft.partial(genre=\"Horror\") | llm | StrOutputParser(),\n",
        ")\n",
        "\n",
        "# 2. The Aggregator (Convergence)\n",
        "prompt_combine = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    I have three movie ideas for the topic '{topic}':\n",
        "    1. Sci-Fi: {draft_scifi}\n",
        "    2. Romance: {draft_romance}\n",
        "    3. Horror: {draft_horror}\n",
        "\n",
        "    Your task: Create a new Mega-Movie that combines the TECHNOLOGY of Sci-Fi, the PASSION of Romance, and the FEAR of Horror.\n",
        "    Write one paragraph.\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 3. The Chain\n",
        "got_chain = (\n",
        "    RunnableParallel(topic=RunnableLambda(lambda x: x), drafts=drafts)\n",
        "    | (lambda x: {**x[\"drafts\"], \"topic\": x[\"topic\"]})\n",
        "    | prompt_combine\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(\"--- Graph of Thoughts (GoT) Result ---\")\n",
        "print(got_chain.invoke(\"Time Travel\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "455cb724",
      "metadata": {
        "id": "455cb724"
      },
      "source": [
        "## 4. Summary & Comparison Table\n",
        "\n",
        "| Method | Structure | Best For... | Cost/Latency |\n",
        "|--------|-----------|-------------|--------------|\n",
        "| **Simple Prompt** | Input -> Output | Simple facts, summaries | ⭐ Low |\n",
        "| **CoT (Chain)** | Input -> Steps -> Output | Math, Logic, Debugging | ⭐⭐ Med |\n",
        "| **ToT (Tree)** | Input -> 3x Branches -> Select -> Output | Strategic decisions, Brainstorming | ⭐⭐⭐ High |\n",
        "| **GoT (Graph)** | Input -> Branch -> Mix/Aggregate -> Output | Creative Writing, Research Synthesis | ⭐⭐⭐⭐ V. High |\n",
        "\n",
        "**Recommendation:** Start with CoT. Only use ToT/GoT if CoT fails."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "jMpr15sYOADP"
      },
      "id": "jMpr15sYOADP",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}